{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of AmazonML2.ipynb","provenance":[{"file_id":"1KSTmE1_mlTljmIqBNVdgzd5adkROiFq-","timestamp":1627823293411}],"collapsed_sections":["aZd6b3hl3Oea"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wu0cKY6Wl4GB","executionInfo":{"status":"ok","timestamp":1627825132930,"user_tz":-330,"elapsed":2784,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"cb1a849c-9d8f-4caf-a7ee-9d7ccbbc854e"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GHcEhhmUl8Ei"},"source":["### Necessary packages"]},{"cell_type":"code","metadata":{"id":"Oue26IXml7w_","executionInfo":{"status":"ok","timestamp":1627825137330,"user_tz":-330,"elapsed":4403,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["import torch \n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader,SubsetRandomSampler\n","import torch.optim as optim\n","\n","import os\n","import copy\n","from collections import defaultdict\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from pylab import rcParams\n","import csv\n","import time\n","from tqdm import tqdm\n","import random\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n","from transformers import AutoTokenizer,AutoModel,AutoModelForSequenceClassification,AdamW,get_linear_schedule_with_warmup\n","\n","seed_val = 42 \n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhDcvnU4JaoU","executionInfo":{"status":"ok","timestamp":1627825137334,"user_tz":-330,"elapsed":17,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"7ed3ff30-cdc7-486e-99a2-1bb15fc1af93"},"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print(\"Running on gpu\",torch.cuda.get_device_name(0))\n","else:\n","    device = 'cpu'\n","    print('No GPU found Running on cpu')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Running on gpu Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5rIYey22l7tU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825137334,"user_tz":-330,"elapsed":12,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"09df2ecb-240c-48da-f1d4-66ef9cfaa103"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SVWO8IGIl7q5","executionInfo":{"status":"ok","timestamp":1627825140433,"user_tz":-330,"elapsed":3106,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["dataset_dir = \"/content/drive/MyDrive/Amazon/dataset/new3.csv\"\n","\n","train_df = pd.read_csv(dataset_dir,index_col=None)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"bA1OlKGbxtUD","executionInfo":{"status":"ok","timestamp":1627825143992,"user_tz":-330,"elapsed":547,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["train_df.drop(['0','Unnamed: 0'],axis=1,inplace=True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Us7oVGbZx7a6","executionInfo":{"status":"ok","timestamp":1627825146326,"user_tz":-330,"elapsed":442,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["train_df.rename(columns={'1': 'TITLE',\n","                   '3': 'BROWSE_NODE_ID'},\n","          inplace=True, errors='raise')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWQ2zfmEyQ2U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825146998,"user_tz":-330,"elapsed":3,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"bfdba7d8-bdd9-441e-ca8b-7783095ceeab"},"source":["train_df.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(264781, 3)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"lLWud-oKyTqM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825154188,"user_tz":-330,"elapsed":1890,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"7af3fcc1-590a-42d7-ad0d-673b810598c1"},"source":["train_df = train_df.drop_duplicates()\n","train_df.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(264781, 3)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"quB5XEQvl7oz","executionInfo":{"status":"ok","timestamp":1627825155044,"user_tz":-330,"elapsed":32,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["train_df = train_df[train_df['TITLE'].notnull()]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKUyDnA2l7Iu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825155045,"user_tz":-330,"elapsed":31,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"c14f17da-90df-4dbf-8958-5319b17998cd"},"source":["train_df['TITLE'].isnull().sum()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"L5Id6SlFTuOj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825155046,"user_tz":-330,"elapsed":25,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"a7deb40e-7061-480f-82b7-121d42200b5c"},"source":["le = LabelEncoder()\n","train_df['BROWSE_NODE_ID'] = le.fit_transform(train_df['BROWSE_NODE_ID'])\n","train_df['BROWSE_NODE_ID'].max()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9907"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"-4SbM_wjxgK8","executionInfo":{"status":"ok","timestamp":1627825155046,"user_tz":-330,"elapsed":23,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["sentences = train_df['TITLE'].values \n","labels = train_df['BROWSE_NODE_ID'].values"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"7GzsJdf1yhiw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825155047,"user_tz":-330,"elapsed":23,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"dc5570d0-292f-4475-cc3b-fd8bd4e7aed8"},"source":["print(sentences.shape,labels.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["(264781,) (264781,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hBEA28h-1M53","executionInfo":{"status":"ok","timestamp":1627825155047,"user_tz":-330,"elapsed":21,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wI7xEuMl1OV2","executionInfo":{"status":"ok","timestamp":1627825155048,"user_tz":-330,"elapsed":21,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"f02ed71d-02a8-40de-efed-709c693c0e34"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YMrrty_Oyi9c","executionInfo":{"status":"ok","timestamp":1627825155048,"user_tz":-330,"elapsed":19,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["train_sentences,val_sentences,train_labels,val_labels = train_test_split(sentences,labels,test_size = 0.1,random_state=seed_val)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFqwJozMy5KS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825155049,"user_tz":-330,"elapsed":19,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"94837f4b-f87a-4904-f069-37ec0755eb48"},"source":["print(f\"No. of training sentences {len(train_sentences)}\")\n","print(f\"No. of validation sentences {len(val_sentences)}\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["No. of training sentences 238302\n","No. of validation sentences 26479\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M5reelgJzgsQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825155049,"user_tz":-330,"elapsed":16,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"f2526130-50d5-4100-b0e3-d329c51132e0"},"source":["train_df.memory_usage(deep= True)*(1e-6)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index               2.118248\n","TITLE             266.718228\n","2                  38.295362\n","BROWSE_NODE_ID      2.118248\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"ur2v7Gg00WLo","executionInfo":{"status":"ok","timestamp":1627825155050,"user_tz":-330,"elapsed":15,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["# indices , cnts = np.unique(labels,return_counts=True)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8LCKQdV136Y","executionInfo":{"status":"ok","timestamp":1627825155050,"user_tz":-330,"elapsed":15,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["# sns.countplot(y = cnts[ (cnts >=10) & (cnts <=100)] )"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"pknP3ajt3GpX","executionInfo":{"status":"ok","timestamp":1627825155050,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["model_name = 'bert-base-multilingual-cased'\n","max_input_length = 128\n","batch_size = 64"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZd6b3hl3Oea"},"source":["### Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"2i-ntoUF2I7X","executionInfo":{"status":"ok","timestamp":1627825157305,"user_tz":-330,"elapsed":2269,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCg52pkq3Km1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157306,"user_tz":-330,"elapsed":54,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"d57d529a-71ba-4d3b-e7d0-d660c2731bcf"},"source":["idx = 100000\n","sample_text = sentences[idx]\n","tokens =tokenizer.tokenize(sample_text)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print('Sample text {}'.format(sample_text))\n","print('Tokens {}'.format(tokens))\n","print('Token IDS {}'.format(token_ids))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"},{"output_type":"stream","text":["Sample text 0.057975 0.026596 -0.010622 0.061523 0.085003 0.045561 -0.025591 -0.11164 -0.078361 0.089149 0.091196 0.078409 0.07734 0.042945 -0.003196 0.04448 -0.01521 -0.059665 -0.00032686 -0.052502 0.047911 -0.0087723 -0.036279 0.025851 0.034307 -0.049543 0.058067 0.061216 0.013107 -0.045237 0.030591 -0.022425 0.031103 -0.0013939 0.034915 -0.030469 -0.0092505 -0.0098639 0.0096247 -0.028933 -0.072186 0.081712 0.03693 -0.051753 0.045296 0.035664 0.067193 -0.023226 0.0229 -0.055481 -0.12465 0.013372 -0.010935 -0.013963 -0.093246 -0.069558 -0.033261 0.032664 0.063967 0.0063827 -0.013734 0.043319 -0.11166 0.029856 0.036687 0.045824 0.054759 0.02211 -0.024031 0.08427 0.10079 0.02853 -0.01582 -0.056987 0.068676 0.1076 -0.020674 -0.12757 0.066519 0.0030416 0.0016169 0.018566 -0.15164 -0.035377 0.0036236 0.054584 0.043646 -0.061214 -0.047801 0.0044398 -0.10973 0.019909 0.070821 0.046859 -0.039432 -0.00045641 0.011947 -0.055183 0.042343 -0.044559 \n","Tokens ['0', '.', '05', '##7', '##97', '##5', '0', '.', '026', '##59', '##6', '-', '0', '.', '010', '##6', '##22', '0', '.', '061', '##52', '##3', '0', '.', '08', '##50', '##0', '##3', '0', '.', '045', '##5', '##6', '##1', '-', '0', '.', '025', '##59', '##1', '-', '0', '.', '111', '##64', '-', '0', '.', '078', '##36', '##1', '0', '.', '08', '##91', '##49', '0', '.', '09', '##11', '##9', '##6', '0', '.', '078', '##40', '##9', '0', '.', '07', '##7', '##34', '0', '.', '042', '##9', '##45', '-', '0', '.', '003', '##19', '##6', '0', '.', '044', '##48', '-', '0', '.', '015', '##21', '-', '0', '.', '05', '##9', '##66', '##5', '-', '0', '.', '000', '##32', '##6', '##86', '-', '0', '.', '052', '##50', '##2', '0', '.', '047', '##91', '##1', '-', '0', '.', '008', '##7', '##7', '##23', '-', '0', '.', '036', '##27', '##9', '0', '.', '025', '##85', '##1', '0', '.', '034', '##30', '##7', '-', '0', '.', '049', '##5', '##43', '0', '.', '05', '##80', '##6', '##7', '0', '.', '061', '##21', '##6', '0', '.', '013', '##10', '##7', '-', '0', '.', '045', '##23', '##7', '0', '.', '030', '##59', '##1', '-', '0', '.', '022', '##42', '##5', '0', '.', '031', '##10', '##3', '-', '0', '.', '001', '##3', '##9', '##3', '##9', '0', '.', '034', '##91', '##5', '-', '0', '.', '030', '##46', '##9', '-', '0', '.', '009', '##25', '##0', '##5', '-', '0', '.', '009', '##86', '##3', '##9', '0', '.', '009', '##6', '##24', '##7', '-', '0', '.', '028', '##9', '##33', '-', '0', '.', '07', '##21', '##86', '0', '.', '08', '##17', '##12', '0', '.', '036', '##9', '##3', '-', '0', '.', '05', '##17', '##53', '0', '.', '045', '##2', '##9', '##6', '0', '.', '035', '##66', '##4', '0', '.', '067', '##19', '##3', '-', '0', '.', '023', '##22', '##6', '0', '.', '022', '##9', '-', '0', '.', '055', '##48', '##1', '-', '0', '.', '1246', '##5', '0', '.', '013', '##3', '##7', '##2', '-', '0', '.', '010', '##9', '##35', '-', '0', '.', '013', '##9', '##6', '##3', '-', '0', '.', '093', '##24', '##6', '-', '0', '.', '06', '##9', '##55', '##8', '-', '0', '.', '033', '##26', '##1', '0', '.', '032', '##66', '##4', '0', '.', '063', '##9', '##6', '##7', '0', '.', '006', '##38', '##27', '-', '0', '.', '013', '##7', '##34', '0', '.', '043', '##31', '##9', '-', '0', '.', '111', '##66', '0', '.', '029', '##85', '##6', '0', '.', '036', '##6', '##8', '##7', '0', '.', '045', '##82', '##4', '0', '.', '054', '##7', '##59', '0', '.', '022', '##11', '-', '0', '.', '024', '##0', '##31', '0', '.', '08', '##42', '##7', '0', '.', '1007', '##9', '0', '.', '028', '##53', '-', '0', '.', '015', '##82', '-', '0', '.', '056', '##9', '##8', '##7', '0', '.', '068', '##6', '##7', '##6', '0', '.', '107', '##6', '-', '0', '.', '020', '##6', '##7', '##4', '-', '0', '.', '1275', '##7', '0', '.', '06', '##65', '##19', '0', '.', '003', '##04', '##16', '0', '.', '001', '##6', '##16', '##9', '0', '.', '018', '##5', '##66', '-', '0', '.', '1516', '##4', '-', '0', '.', '035', '##3', '##7', '##7', '0', '.', '003', '##6', '##23', '##6', '0', '.', '054', '##58', '##4', '0', '.', '043', '##64', '##6', '-', '0', '.', '061', '##21', '##4', '-', '0', '.', '047', '##80', '##1', '0', '.', '004', '##43', '##9', '##8', '-', '0', '.', '1097', '##3', '0', '.', '019', '##90', '##9', '0', '.', '070', '##82', '##1', '0', '.', '046', '##85', '##9', '-', '0', '.', '039', '##43', '##2', '-', '0', '.', '000', '##45', '##64', '##1', '0', '.', '011', '##9', '##47', '-', '0', '.', '055', '##18', '##3', '0', '.', '042', '##34', '##3', '-', '0', '.', '044', '##55', '##9']\n","Token IDS [121, 119, 10831, 11305, 100595, 11166, 121, 119, 75302, 108710, 11211, 118, 121, 119, 49470, 11211, 71793, 121, 119, 106084, 92161, 10884, 121, 119, 11052, 28847, 10929, 10884, 121, 119, 98603, 11166, 11211, 10759, 118, 121, 119, 73887, 108710, 10759, 118, 121, 119, 15821, 51658, 118, 121, 119, 107573, 60878, 10759, 121, 119, 11052, 74178, 99808, 121, 119, 11035, 37115, 11373, 11211, 121, 119, 107573, 38109, 11373, 121, 119, 10878, 11305, 78301, 121, 119, 97672, 11373, 76977, 118, 121, 119, 26861, 54055, 11211, 121, 119, 101679, 32168, 118, 121, 119, 75737, 47499, 118, 121, 119, 10831, 11373, 87372, 11166, 118, 121, 119, 10259, 68430, 11211, 103450, 118, 121, 119, 103993, 28847, 10729, 121, 119, 108614, 74178, 10759, 118, 121, 119, 54307, 11305, 11305, 74171, 118, 121, 119, 109077, 90861, 11373, 121, 119, 73887, 69975, 10759, 121, 119, 103542, 32792, 11305, 118, 121, 119, 106353, 11166, 49469, 121, 119, 10831, 44026, 11211, 11305, 121, 119, 106084, 47499, 11211, 121, 119, 64489, 20305, 11305, 118, 121, 119, 98603, 74171, 11305, 121, 119, 78336, 108710, 10759, 118, 121, 119, 83983, 91147, 11166, 121, 119, 95431, 20305, 10884, 118, 121, 119, 17449, 10884, 11373, 10884, 11373, 121, 119, 103542, 74178, 11166, 118, 121, 119, 78336, 82049, 11373, 118, 121, 119, 58526, 69168, 10929, 11166, 118, 121, 119, 58526, 103450, 10884, 11373, 121, 119, 58526, 11211, 53398, 11305, 118, 121, 119, 90262, 11373, 69646, 118, 121, 119, 10878, 47499, 103450, 121, 119, 11052, 34264, 24747, 121, 119, 109077, 11373, 10884, 118, 121, 119, 10831, 34264, 67574, 121, 119, 98603, 10729, 11373, 11211, 121, 119, 98889, 87372, 11011, 121, 119, 107401, 54055, 10884, 118, 121, 119, 84786, 71793, 11211, 121, 119, 83983, 11373, 118, 121, 119, 79262, 32168, 10759, 118, 121, 119, 96308, 11166, 121, 119, 64489, 10884, 11305, 10729, 118, 121, 119, 49470, 11373, 76897, 118, 121, 119, 64489, 11373, 11211, 10884, 118, 121, 119, 109512, 53398, 11211, 118, 121, 119, 10719, 11373, 99555, 11396, 118, 121, 119, 100402, 90533, 10759, 121, 119, 104038, 87372, 11011, 121, 119, 100445, 11373, 11211, 11305, 121, 119, 52076, 78533, 90861, 118, 121, 119, 64489, 11305, 78301, 121, 119, 104367, 81456, 11373, 118, 121, 119, 15821, 87372, 121, 119, 97567, 69975, 11211, 121, 119, 109077, 11211, 11396, 11305, 121, 119, 98603, 78675, 11011, 121, 119, 70967, 11305, 108710, 121, 119, 83983, 37115, 118, 121, 119, 81201, 10929, 81456, 121, 119, 11052, 91147, 11305, 121, 119, 37308, 11373, 121, 119, 90262, 67574, 118, 121, 119, 75737, 78675, 118, 121, 119, 101451, 11373, 11396, 11305, 121, 119, 102517, 11211, 11305, 11211, 121, 119, 15844, 11211, 118, 121, 119, 71906, 11211, 11305, 11011, 118, 121, 119, 78350, 11305, 121, 119, 10719, 74327, 54055, 121, 119, 26861, 92546, 37301, 121, 119, 17449, 11211, 37301, 11373, 121, 119, 82446, 11166, 87372, 118, 121, 119, 49393, 11011, 118, 121, 119, 98889, 10884, 11305, 11305, 121, 119, 26861, 11211, 74171, 11211, 121, 119, 70967, 83148, 11011, 121, 119, 104367, 51658, 11211, 118, 121, 119, 106084, 47499, 11011, 118, 121, 119, 108614, 44026, 10759, 121, 119, 44827, 49469, 11373, 11396, 118, 121, 119, 60882, 10884, 121, 119, 85444, 61400, 11373, 121, 119, 89041, 78675, 10759, 121, 119, 77305, 69975, 11373, 118, 121, 119, 96241, 49469, 10729, 118, 121, 119, 10259, 76977, 51658, 10759, 121, 119, 62384, 11373, 110715, 118, 121, 119, 79262, 45987, 10884, 121, 119, 97672, 78301, 10884, 118, 121, 119, 101679, 99555, 11373]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qcgFU6Aj5hao","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157307,"user_tz":-330,"elapsed":47,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"6fac92ac-6134-41d3-9e30-39114df60aff"},"source":["tokenizer.sep_token,tokenizer.sep_token_id"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('[SEP]', 102)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"H_qmWHfS6KW6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157308,"user_tz":-330,"elapsed":42,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"899e4362-5203-49f7-9946-a47d3fb4b2a4"},"source":["tokenizer.cls_token,tokenizer.cls_token_id"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('[CLS]', 101)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"MzvFCZNT6ONQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157309,"user_tz":-330,"elapsed":41,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"f6e5e491-2b96-40de-dddc-b9b442a8b526"},"source":["tokenizer.pad_token,tokenizer.pad_token_id"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('[PAD]', 0)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"x_P7s9I96RQ_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157311,"user_tz":-330,"elapsed":40,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"f68db9b6-7bab-4a9e-9f16-d8e716dd79ff"},"source":["tokenizer.unk_token,tokenizer.unk_token_id"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('[UNK]', 100)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"yGWRjGfb6ma4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157312,"user_tz":-330,"elapsed":38,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"e8f469ce-9ca7-412d-9c33-94a3fe786e0d"},"source":["encoding = tokenizer.encode_plus(\n","    sample_text,\n","    max_length = max_input_length,\n","    add_special_tokens = True,\n","    pad_to_max_length=True,\n","    return_attention_mask = True,\n","    return_token_type_ids = False,\n","    return_tensors = 'pt'\n",")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Gu9om6It7rtU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157315,"user_tz":-330,"elapsed":36,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"abed26f5-ad49-42d1-bff9-655fbdc843eb"},"source":["encoding"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[   101,    121,    119,  10831,  11305, 100595,  11166,    121,    119,\n","          75302, 108710,  11211,    118,    121,    119,  49470,  11211,  71793,\n","            121,    119, 106084,  92161,  10884,    121,    119,  11052,  28847,\n","          10929,  10884,    121,    119,  98603,  11166,  11211,  10759,    118,\n","            121,    119,  73887, 108710,  10759,    118,    121,    119,  15821,\n","          51658,    118,    121,    119, 107573,  60878,  10759,    121,    119,\n","          11052,  74178,  99808,    121,    119,  11035,  37115,  11373,  11211,\n","            121,    119, 107573,  38109,  11373,    121,    119,  10878,  11305,\n","          78301,    121,    119,  97672,  11373,  76977,    118,    121,    119,\n","          26861,  54055,  11211,    121,    119, 101679,  32168,    118,    121,\n","            119,  75737,  47499,    118,    121,    119,  10831,  11373,  87372,\n","          11166,    118,    121,    119,  10259,  68430,  11211, 103450,    118,\n","            121,    119, 103993,  28847,  10729,    121,    119, 108614,  74178,\n","          10759,    118,    121,    119,  54307,  11305,  11305,  74171,    118,\n","            121,    102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]])}"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"IxXhFChq7suv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825157316,"user_tz":-330,"elapsed":33,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"0b095630-7f70-4a90-a4e7-b95d33225590"},"source":["encoding.keys()"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'attention_mask'])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"xdvBrtuoRM8V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825160172,"user_tz":-330,"elapsed":2884,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"b2a00235-a025-4509-8797-d49310a67fe4"},"source":["base_model = AutoModel.from_pretrained(model_name)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rpiW_ZXORSIs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825160173,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"01d262f1-9576-43db-b6ff-9580735a02c9"},"source":["base_model(**encoding)['pooler_output']"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 5.1511e-02,  6.5430e-02,  1.1095e-01,  9.7697e-02,  8.1404e-02,\n","          1.1452e-01,  1.8644e-02,  1.4287e-01, -8.6418e-02,  5.2267e-02,\n","          1.5464e-01, -1.2392e-02,  5.3117e-02, -3.7797e-02,  1.1085e-01,\n","          1.2588e-02,  8.3484e-02, -7.0417e-02, -3.5017e-02, -5.4347e-02,\n","         -9.9998e-01, -2.6268e-02,  1.0919e-01, -1.7575e-02, -1.4697e-01,\n","         -1.2892e-01,  5.2459e-02, -3.4963e-02, -1.0806e-02,  8.1087e-02,\n","         -1.2304e-01, -9.9997e-01, -2.7054e-01,  1.1235e-01, -2.7089e-03,\n","          7.0055e-03, -7.7862e-02,  3.2846e-03,  4.5763e-02, -2.1460e-01,\n","         -3.2374e-02, -8.4805e-02,  7.0363e-02, -3.5499e-03, -2.8374e-03,\n","         -4.9908e-02, -8.6526e-02,  9.8377e-03, -1.3654e-01,  9.1327e-02,\n","          1.1719e-01,  7.8216e-02,  2.1621e-01,  9.7141e-02, -1.3358e-03,\n","         -1.2015e-01,  7.6611e-02,  5.7900e-02,  1.0300e-01,  8.0755e-02,\n","         -6.0743e-02, -2.6530e-02, -8.9517e-02, -7.0763e-03, -3.2200e-02,\n","         -1.0450e-01, -8.8776e-02,  4.3160e-02,  1.9369e-01,  2.8788e-02,\n","          1.5866e-03, -2.3207e-01, -1.3137e-02,  3.1957e-02, -4.5573e-02,\n","         -7.1311e-02,  5.2214e-02,  1.2168e-01, -7.2928e-02,  5.6402e-02,\n","         -1.1683e-02, -9.7543e-02, -4.8009e-02, -4.5485e-02,  9.0053e-02,\n","          7.6172e-02,  8.0033e-02, -8.2138e-02,  4.4587e-02,  7.8235e-02,\n","         -8.6311e-02,  1.7738e-01, -3.9430e-02,  1.2278e-01,  1.3646e-02,\n","          1.7158e-02, -7.1078e-02, -4.0338e-04,  2.0917e-02, -1.1843e-01,\n","          1.0039e-01, -5.2465e-02, -2.3597e-02,  3.2818e-02, -4.5006e-02,\n","          2.2490e-02,  3.3182e-02,  1.1960e-01,  6.6878e-04,  1.4977e-01,\n","         -7.0185e-02, -1.0461e-01, -8.1331e-02, -4.7971e-02,  1.3289e-02,\n","          1.7372e-01,  1.8968e-01, -1.9002e-02, -2.3318e-01,  1.6399e-02,\n","         -2.1591e-01,  9.9998e-01,  2.4951e-02,  9.6635e-03, -7.1613e-02,\n","         -3.5640e-02, -1.9217e-01,  2.7223e-02,  8.8258e-02,  1.0702e-01,\n","         -6.0371e-02,  7.2998e-02,  8.3854e-02, -1.6314e-01, -1.0564e-01,\n","         -2.8319e-02,  1.7482e-02,  3.3685e-03, -7.2175e-02, -6.6346e-02,\n","          5.6253e-02,  1.9345e-01, -1.6103e-01, -5.4476e-03,  5.9785e-02,\n","          6.1632e-02, -8.1479e-02,  9.3478e-03,  9.9997e-01,  1.0552e-01,\n","          7.1148e-02, -1.1382e-02, -1.7140e-01, -2.5152e-01, -1.5016e-01,\n","         -1.2885e-01, -3.5559e-02, -8.5639e-02,  3.9239e-02,  8.3639e-02,\n","         -5.2847e-02,  1.4202e-02, -4.1472e-02, -2.7075e-02,  3.5133e-02,\n","         -2.3605e-01, -8.4137e-02,  1.1034e-01,  5.9835e-02,  8.8273e-02,\n","          8.5056e-02,  9.2647e-02,  2.9404e-03, -1.2325e-02,  1.0535e-01,\n","          7.0362e-02,  7.5587e-03,  6.9521e-02,  1.2361e-01, -1.3999e-02,\n","          1.7163e-03,  1.5716e-01,  1.7517e-01, -7.2868e-02,  5.4072e-02,\n","         -1.6591e-01,  7.5837e-02,  6.2990e-02,  5.9626e-02, -2.2683e-02,\n","          6.8205e-02,  5.1026e-02, -1.3747e-01, -6.0964e-02, -7.5470e-03,\n","          1.1565e-02, -1.5932e-01,  3.2028e-02,  1.9545e-02,  1.1641e-01,\n","         -5.8541e-02, -1.2424e-02, -7.4744e-02, -5.1018e-02, -3.9306e-02,\n","          3.2110e-01, -1.1046e-01,  7.8734e-02,  1.5166e-01, -3.0966e-02,\n","         -2.2881e-01, -5.7660e-02,  1.9613e-01,  2.9440e-02,  7.9552e-03,\n","          1.3505e-02,  6.0999e-02,  6.8204e-02,  3.8135e-03,  2.6367e-02,\n","          3.4358e-03, -6.6888e-02,  4.4250e-04,  5.5364e-02,  4.8656e-02,\n","          1.0716e-01,  1.6738e-02,  2.3325e-02, -2.9375e-02,  6.4736e-02,\n","          4.2977e-02,  4.9693e-02, -9.2708e-02,  8.1460e-02,  8.0652e-02,\n","          2.8089e-01, -7.8731e-02, -4.5731e-02, -1.7210e-01,  2.4364e-03,\n","          4.2777e-02, -1.5223e-02,  3.2366e-02,  1.2077e-01, -1.3590e-01,\n","         -1.0806e-02,  3.4829e-02,  9.0609e-02, -5.8759e-02, -4.7977e-02,\n","         -1.3225e-01, -1.9954e-01, -1.7097e-01, -9.5088e-02, -5.0936e-02,\n","         -9.9997e-01, -2.4683e-03, -1.2323e-02,  1.5161e-01, -5.4993e-03,\n","          6.9105e-03,  1.0533e-01, -2.5417e-02,  1.9414e-01, -1.5291e-01,\n","         -1.6768e-01,  2.4417e-02,  4.5200e-02, -1.8746e-01,  2.2749e-02,\n","          9.8800e-02, -1.4094e-01,  4.1013e-02,  1.1811e-01,  4.5552e-02,\n","         -1.0227e-02,  1.1945e-01, -1.4719e-01,  1.3312e-01,  4.7110e-02,\n","          1.4920e-02, -5.0830e-02,  1.9781e-02, -9.9997e-01,  1.3101e-01,\n","         -6.0858e-04,  4.9761e-02,  7.5759e-02, -1.1492e-01, -7.0962e-02,\n","         -6.6143e-02,  1.8764e-01,  8.0132e-02, -2.0190e-02,  2.3535e-02,\n","          2.5526e-01,  1.0301e-01, -5.4444e-03, -3.3212e-02,  3.2427e-02,\n","          1.3066e-01,  7.4611e-02, -4.3294e-02,  1.5387e-01, -1.3054e-02,\n","          7.0493e-02, -3.8779e-03,  9.5894e-03,  9.3778e-02, -3.1544e-02,\n","         -1.1697e-01, -6.3113e-03, -2.0216e-02, -2.0097e-01, -3.4014e-02,\n","         -6.4012e-02,  1.1563e-01,  4.0339e-02, -1.6067e-02,  7.6383e-02,\n","         -1.3412e-01,  2.7486e-04,  4.9363e-02,  9.9997e-01, -7.2245e-03,\n","          5.4962e-02,  1.8457e-01,  2.4147e-01, -9.6283e-02, -2.3639e-02,\n","          3.7811e-01, -2.4206e-02,  2.3735e-01,  1.3738e-01, -4.4943e-02,\n","         -5.2251e-02, -1.2118e-01,  7.2826e-02, -9.0324e-02, -4.9386e-02,\n","         -4.4244e-03, -1.0154e-01, -8.2577e-03,  1.0917e-01, -1.5255e-01,\n","          2.0759e-02, -3.9816e-02,  9.0537e-02,  2.0383e-01, -2.6297e-02,\n","         -9.6784e-02, -5.9404e-02, -9.1829e-03,  3.4222e-02, -3.2693e-02,\n","          2.4028e-01,  8.1289e-02,  6.1413e-02, -5.1480e-02, -3.0242e-02,\n","         -3.0066e-02, -1.0170e-01, -1.1096e-01, -1.3971e-02, -9.9548e-03,\n","         -1.6815e-01,  8.4415e-04,  7.7198e-02,  1.1947e-01, -1.1074e-01,\n","          1.5150e-01, -9.9997e-01, -7.9515e-02,  5.5698e-02, -3.7160e-02,\n","          4.9072e-03, -6.3799e-02,  1.7423e-02,  6.8971e-02,  4.4573e-02,\n","          6.3372e-02, -1.1195e-01, -6.9114e-02, -7.9710e-02,  1.0318e-01,\n","         -6.9501e-02,  2.5171e-01,  2.2738e-01, -8.5449e-02, -4.4331e-02,\n","         -3.9833e-02, -1.9350e-01,  1.5173e-02,  9.8248e-02, -3.5899e-02,\n","         -9.2368e-02,  3.4035e-02, -7.8364e-02, -4.0123e-02,  1.2696e-01,\n","         -5.6603e-02,  3.3229e-02, -1.1884e-02, -4.0795e-02,  4.3913e-02,\n","         -8.7340e-03,  1.7935e-01,  1.0575e-01,  1.5146e-02, -2.9881e-02,\n","          5.0742e-02,  6.8436e-02, -8.3205e-02,  4.2260e-02, -1.4350e-01,\n","          2.0668e-01, -5.5423e-02,  2.0634e-01, -8.0872e-02,  9.5068e-03,\n","          1.1729e-01,  9.9998e-01,  1.5783e-01,  6.5191e-02,  5.6408e-02,\n","         -1.6123e-02,  3.5088e-03, -1.6793e-01,  4.0107e-02,  4.0471e-02,\n","          2.4739e-02, -2.5445e-02, -1.3023e-01, -1.3072e-02,  1.2827e-01,\n","          1.4285e-01, -1.6754e-01, -1.4094e-01,  6.0273e-02,  1.3741e-01,\n","          6.0117e-03, -1.6449e-01, -9.6921e-02,  7.0901e-02,  3.6343e-02,\n","         -8.5389e-02,  2.1720e-01, -1.3442e-02, -6.8504e-02,  4.1603e-02,\n","          6.0738e-02, -1.1839e-01,  1.9993e-02,  8.1127e-02,  8.3623e-03,\n","         -2.1230e-01,  9.9997e-01,  1.1606e-01,  5.7643e-02,  1.0060e-01,\n","          5.1670e-02,  5.6742e-02,  4.9570e-02,  1.4304e-01,  4.7029e-02,\n","         -3.4829e-03,  1.5771e-02, -1.2126e-01,  1.2378e-02,  4.5210e-03,\n","          2.3509e-01,  2.9701e-02,  7.0389e-02, -1.1286e-01,  1.0975e-01,\n","          1.5430e-01,  2.0623e-02, -9.4410e-02, -9.0392e-03, -4.3740e-02,\n","          1.9863e-01, -1.5213e-02,  2.0513e-02, -2.1641e-02, -2.7884e-02,\n","          9.6008e-02,  3.2868e-02, -1.0657e-01,  5.2947e-02,  1.7113e-02,\n","         -8.4392e-02, -5.4379e-02, -6.2047e-02, -1.8380e-01,  9.9997e-01,\n","          1.1678e-01, -4.9668e-03, -6.4486e-03,  2.5229e-02, -8.6156e-02,\n","          3.9800e-02,  1.5138e-01, -3.2076e-02,  6.9522e-03, -5.0884e-03,\n","         -2.5013e-01, -9.1914e-03,  5.8269e-02,  3.3187e-01, -4.1177e-03,\n","          1.5613e-01, -1.8923e-02,  4.8695e-02, -2.3891e-02, -5.4376e-02,\n","         -2.4775e-01,  1.3477e-02,  7.4355e-02,  8.0662e-02,  5.3587e-02,\n","          1.2902e-01, -9.6537e-03, -1.6987e-02, -8.4679e-02,  1.1908e-02,\n","          9.9997e-01,  9.9997e-01, -1.0691e-02,  4.4028e-02, -8.1562e-02,\n","         -7.3625e-02, -5.1502e-02,  1.0861e-01,  1.0762e-01, -1.9250e-02,\n","         -5.5131e-02, -1.1224e-01, -1.1749e-01, -2.0728e-02,  4.7366e-02,\n","          4.8749e-02, -1.9686e-02,  2.7963e-02,  2.5596e-03,  8.6063e-02,\n","          1.1268e-01, -1.0704e-01,  1.6944e-01, -6.7776e-02,  3.8304e-02,\n","          6.5229e-02,  1.0521e-02,  1.7060e-01,  5.6323e-02,  9.5076e-02,\n","         -1.4673e-01, -8.1755e-02, -9.9996e-01,  2.0169e-02, -5.3956e-02,\n","         -6.3187e-02,  1.1891e-01,  3.5832e-02, -8.9724e-02, -3.7457e-02,\n","          3.5850e-02, -2.0719e-02, -1.4648e-02, -3.7488e-02, -8.0144e-02,\n","          4.6173e-02, -1.1411e-01,  4.1636e-02, -3.8875e-02, -6.3046e-02,\n","         -1.5277e-03, -4.7571e-02, -6.4690e-02, -2.0767e-02, -1.1190e-02,\n","          9.7223e-02,  1.5921e-01,  3.5868e-02, -2.0155e-02,  5.2469e-02,\n","          2.5045e-02, -6.8141e-02,  5.0934e-02, -1.9270e-02,  7.5973e-02,\n","          4.0257e-02,  5.6607e-02, -4.4491e-02,  3.5415e-02, -4.2546e-02,\n","          4.9040e-02,  1.0317e-01, -7.4704e-02, -2.3179e-02, -2.0636e-02,\n","          1.8397e-02,  9.7048e-02, -4.4051e-02, -7.9142e-02, -2.9561e-02,\n","          8.2235e-02,  1.4637e-01, -1.1450e-02, -5.1515e-02, -8.1813e-03,\n","          9.8633e-02,  1.9066e-01, -8.0244e-02,  8.5848e-02, -2.7521e-02,\n","          9.9998e-01,  7.6191e-02,  6.6460e-02,  1.8054e-02,  6.4140e-03,\n","          1.0779e-01, -1.3811e-03,  8.2497e-02, -6.7930e-02, -3.4218e-01,\n","         -5.3555e-03,  3.5515e-02, -3.0467e-02,  5.6184e-02, -4.6251e-02,\n","          3.1759e-02,  1.5893e-01,  1.5639e-01,  8.0622e-03, -5.2766e-02,\n","          1.9659e-02, -1.1319e-02,  7.0921e-03,  4.5631e-02,  7.0155e-02,\n","         -1.0387e-02,  6.7415e-03,  7.6464e-02,  2.6562e-02,  8.6797e-02,\n","          5.7834e-02,  6.4648e-02, -4.6619e-02, -9.7221e-02,  6.6077e-02,\n","          7.6577e-02,  6.6445e-02, -3.9068e-02,  6.0587e-02, -3.7171e-02,\n","          1.2916e-01, -4.8888e-02, -3.8605e-02,  1.6803e-01, -1.4280e-01,\n","          1.5250e-02, -2.7446e-03,  3.7378e-02, -7.2886e-02, -9.3678e-02,\n","         -2.2546e-02, -9.9187e-02, -4.5370e-02,  1.3647e-01, -4.8468e-02,\n","         -8.0249e-02,  8.1442e-02,  5.4598e-02,  8.9668e-02,  4.4441e-02,\n","          1.1838e-01, -5.0184e-02,  1.9805e-02, -9.9997e-01,  5.4299e-03,\n","         -9.4151e-02, -3.7495e-02, -1.4269e-01, -2.0803e-02,  4.3258e-02,\n","         -3.4524e-02, -4.6626e-02,  1.5283e-01,  5.7002e-02,  9.2502e-02,\n","         -5.4572e-02, -2.5367e-01, -7.8856e-02, -1.1217e-01, -5.8628e-02,\n","         -1.1586e-01,  7.3670e-02, -2.8910e-02,  2.4598e-02,  4.1084e-02,\n","         -4.8957e-02,  7.2291e-02, -3.3084e-02,  1.3873e-01, -9.7263e-03,\n","         -1.6095e-01, -4.3398e-03,  7.7517e-02, -1.2230e-01, -2.0403e-02,\n","         -1.5812e-02, -9.1621e-02,  3.1307e-02, -9.1097e-02,  4.5340e-03,\n","         -4.4048e-02,  1.1071e-01,  3.5574e-02,  2.8741e-02, -1.2394e-01,\n","         -2.9003e-02, -2.0881e-01, -8.4849e-03,  2.4050e-02, -3.6801e-02,\n","          5.1045e-02,  8.1748e-02, -1.2130e-01,  3.6573e-02,  5.2353e-02,\n","         -4.7322e-02,  2.4963e-02, -7.6527e-02,  8.0986e-02, -5.5080e-02,\n","         -5.4407e-02, -6.3863e-02,  1.1834e-01,  1.4335e-02, -5.1559e-02,\n","         -9.5860e-02, -9.9403e-02, -3.5607e-02,  1.0492e-01,  1.0679e-01,\n","          7.2847e-04,  8.8433e-02, -2.6725e-02,  1.8855e-02, -7.9898e-02,\n","         -4.6796e-02, -1.3293e-01, -3.7341e-02, -6.7906e-02, -1.1136e-01,\n","          1.7185e-01, -2.9816e-01,  6.8683e-03, -2.8989e-02,  1.2088e-01,\n","         -3.5108e-02, -2.9816e-01,  7.8430e-02,  3.6389e-02,  3.0471e-02,\n","         -4.2428e-02, -4.2672e-02,  6.6993e-02]], grad_fn=<TanhBackward>)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"CbDzGp5o8e2v"},"source":["## Choosing token length"]},{"cell_type":"code","metadata":{"id":"pl1hW5vd7z51","executionInfo":{"status":"ok","timestamp":1627825160175,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["# token_lens = []\n","# for txt in sentences:\n","#     tokens = tokenizer.encode(txt,max_length=512)\n","#     token_lens.append(len(tokens))"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-zSVzE18xzF","executionInfo":{"status":"ok","timestamp":1627825160175,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["# sns.displot(token_lens)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZqbay3_81xn","executionInfo":{"status":"ok","timestamp":1627825160176,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["class AmazonDataset(Dataset):\n","\n","  def __init__(self, sentences, labels, tokenizer, max_length,with_labels=True):\n","    self.sentences = sentences\n","    self.labels = labels\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","    self.with_labels = with_labels\n","  \n","  def __len__(self):\n","    return len(self.sentences)\n","  \n","  def __getitem__(self, idx):\n","    sentence = str(self.sentences[idx])\n","    encoding = self.tokenizer.encode_plus(\n","      sentence,\n","      add_special_tokens=True,\n","      max_length=self.max_length,\n","      return_token_type_ids=False,\n","      pad_to_max_length=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","\n","    if self.with_labels:\n","        \n","        label = self.labels[idx]\n","\n","        return {\n","            'sentence': sentence,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","    else:\n","        return {\n","            'sentence': sentence,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","        }"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9NnNAOs_VXd","executionInfo":{"status":"ok","timestamp":1627825160177,"user_tz":-330,"elapsed":11,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["def create_data_loaders(sentences,labels,tokenizer,max_input_length,batch_size,with_labels):\n","    ds = AmazonDataset(\n","        sentences =sentences,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_length=max_input_length,\n","        with_labels = with_labels\n","    )\n","\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size\n","    )"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hyta-on7CL5j","executionInfo":{"status":"ok","timestamp":1627825160177,"user_tz":-330,"elapsed":11,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["train_loader = create_data_loaders(\n","    train_sentences,\n","    train_labels,\n","    tokenizer,\n","    max_input_length=max_input_length,\n","    batch_size=batch_size,\n","    with_labels = True\n",")\n","\n","val_loader = create_data_loaders(\n","    val_sentences,\n","    val_labels,\n","    tokenizer,\n","    max_input_length=max_input_length,\n","    batch_size=batch_size,\n","    with_labels = True\n",")"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"7t9mcWUOId8b","executionInfo":{"status":"ok","timestamp":1627825160178,"user_tz":-330,"elapsed":11,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["class AmazonClassifier(nn.Module):\n","\n","  def __init__(self,base_model_name, n_classes):\n","    super(AmazonClassifier, self).__init__()\n","    self.base_model = AutoModel.from_pretrained(base_model_name)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.base_model.config.hidden_size, n_classes)\n","  \n","  def forward(self, input_ids, attention_mask):\n","    pooled_output = self.base_model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )['pooler_output']\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXXmbmlAI6Bb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825163208,"user_tz":-330,"elapsed":3041,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"6ca43b20-9752-4c2a-92a0-2d4f758253d2"},"source":["model = AmazonClassifier(base_model_name=model_name,n_classes=len(np.unique(labels)))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pw0Pw3VcJD7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627825166124,"user_tz":-330,"elapsed":2920,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}},"outputId":"9f2ad231-9915-4d91-e713-00bcbbb95072"},"source":["model.to(device)"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AmazonClassifier(\n","  (base_model): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (drop): Dropout(p=0.3, inplace=False)\n","  (out): Linear(in_features=768, out_features=9908, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"R46XeyKCJHaA","executionInfo":{"status":"ok","timestamp":1627825166125,"user_tz":-330,"elapsed":18,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["num_epochs = 3\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","\n","total_steps = len(train_loader) * num_epochs\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"w12Iv6sxJSLG","executionInfo":{"status":"ok","timestamp":1627825166126,"user_tz":-330,"elapsed":18,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["def train_epoch(\n","  model, \n","  data_loader, \n","  loss_fn, \n","  optimizer, \n","  device, \n","  scheduler, \n","  n_examples\n","):\n","  model = model.train()\n","\n","  losses = []\n","  correct_predictions = 0\n","  \n","  for i,d in enumerate(data_loader):\n","    if i%100 == 0:\n","        print(f\"Processing batch {i+1}/{len(data_loader)}\")\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    labels = d[\"labels\"].to(device)\n","\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, labels)\n","\n","    correct_predictions += torch.sum(preds == labels)\n","    # print(loss)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"3I1wdUl1JvLw","executionInfo":{"status":"ok","timestamp":1627825166127,"user_tz":-330,"elapsed":18,"user":{"displayName":"Prasanna Devadiga","photoUrl":"","userId":"14898316107755878758"}}},"source":["def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  losses = []\n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for i,d in enumerate(data_loader):\n","      if i%100 == 0:\n","          print(f\"Processing batch {i+1}/{len(data_loader)}\")\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      labels = d[\"labels\"].to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      loss = loss_fn(outputs, labels)\n","\n","      correct_predictions += torch.sum(preds == labels)\n","      losses.append(loss.item())\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhZqTN1TJxl3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0fba0669-b153-416d-d4dc-7369830a932a"},"source":["%%time\n","\n","history = defaultdict(list)\n","best_accuracy = 0\n","\n","for epoch in tqdm(range(num_epochs)):\n","\n","  print(f'Epoch {epoch + 1}/{num_epochs}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_loader,    \n","    loss_fn, \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(train_labels)\n","  )\n","\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","  val_acc, val_loss = eval_model(\n","    model,\n","    val_loader,\n","    loss_fn, \n","    device, \n","    len(val_labels)\n","  )\n","\n","  print(f'Val   loss {val_loss} accuracy {val_acc}')\n","  print()\n","\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['val_acc'].append(val_acc)\n","  history['val_loss'].append(val_loss)\n","\n","  if val_acc > best_accuracy:\n","    torch.save(model.state_dict(), 'best_model_state.bin')\n","    best_accuracy = val_acc"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/3\n","----------\n","Processing batch 1/3724\n","Processing batch 101/3724\n","Processing batch 201/3724\n","Processing batch 301/3724\n","Processing batch 401/3724\n","Processing batch 501/3724\n","Processing batch 601/3724\n","Processing batch 701/3724\n","Processing batch 801/3724\n","Processing batch 901/3724\n","Processing batch 1001/3724\n","Processing batch 1101/3724\n","Processing batch 1201/3724\n","Processing batch 1301/3724\n","Processing batch 1401/3724\n","Processing batch 1501/3724\n","Processing batch 1601/3724\n","Processing batch 1701/3724\n","Processing batch 1801/3724\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ZiIvRraJ0eE"},"source":["plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mcVYdi7sN1mj"},"source":["def get_predictions(model, data_loader):\n","  model = model.eval()\n","  \n","  sentences = []\n","  predictions = []\n","  prediction_probs = []\n","  real_values = []\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","\n","      semtemces = d[\"sentence\"]\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      labels = d[\"labels\"].to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      probs = F.softmax(outputs, dim=1)\n","\n","      texts.extend(texts)\n","      predictions.extend(preds)\n","      prediction_probs.extend(probs)\n","      real_values.extend(labels)\n","\n","  predictions = torch.stack(predictions).cpu()\n","  prediction_probs = torch.stack(prediction_probs).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return review_texts, predictions, prediction_probs, real_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86fTGiDOTVr9"},"source":[""],"execution_count":null,"outputs":[]}]}